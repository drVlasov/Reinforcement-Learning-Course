{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-fY_538-h5Nl"
   },
   "source": [
    "# REINFORCE in TensorFlow\n",
    "\n",
    "Just like we did before for Q-learning, this time we'll design a TensorFlow network to learn `CartPole-v0` via policy gradient (REINFORCE).\n",
    "\n",
    "Most of the code in this notebook is taken from approximate Q-learning, so you'll find it more or less familiar and even simpler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "hHUsTq18h5N5"
   },
   "outputs": [],
   "source": [
    "import sys, os\n",
    "if 'google.colab' in sys.modules:\n",
    "    %tensorflow_version 1.x\n",
    "    \n",
    "    if not os.path.exists('.setup_complete'):\n",
    "        !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/setup_colab.sh -O- | bash\n",
    "\n",
    "        !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/coursera/grading.py -O ../grading.py\n",
    "        !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/coursera/week5_policy_based/submit.py\n",
    "\n",
    "        !touch .setup_complete\n",
    "\n",
    "# This code creates a virtual display to draw game images on.\n",
    "# It will have no effect if your machine has a monitor.\n",
    "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\")) == 0:\n",
    "    !bash ../xvfb start\n",
    "    os.environ['DISPLAY'] = ':1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "tPZDjsAVh5N9"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ir2p-xabh5N_"
   },
   "source": [
    "A caveat: with some versions of `pyglet`, the following cell may crash with `NameError: name 'base' is not defined`. The corresponding bug report is [here](https://github.com/pyglet/pyglet/issues/134). If you see this error, try restarting the kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "id": "kTPMi330h5OA",
    "outputId": "656442d4-fa85-4ee4-a272-39fdd3e7d4a3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f6b396d6750>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAATDElEQVR4nO3dfYyd5Znf8e8P2xgSkhjCrOPaBrMbNynbNiadElCyKkvILkFpyUppBG0JipC8lRwpSFFb2ErdRFqkXaUbWtQtqiNonCYNoZtksSgtC4R2N38EMMRxAIfFCU5ty8bmNeSFF4+v/jG3ycEvzPG8ML5nvh/paJ7neu7nnOtWDr88vuc5c1JVSJL6ccJsNyBJOjYGtyR1xuCWpM4Y3JLUGYNbkjpjcEtSZ2YsuJNcnOSxJNuSXDNTryNJ801m4j7uJAuAvwE+BOwEHgAur6pHp/3FJGmemakr7nOBbVX146p6GbgFuHSGXkuS5pWFM/S8y4EdA/s7gfcdbfDpp59eq1atmqFWJKk/27dv56mnnsqRjs1UcE8oyVpgLcAZZ5zBpk2bZqsVSTrujI6OHvXYTC2V7AJWDuyvaLVXVdX6qhqtqtGRkZEZakOS5p6ZCu4HgNVJzkpyInAZsHGGXkuS5pUZWSqpqv1JPgXcCSwAbq6qR2bitSRpvpmxNe6qugO4Y6aeX5LmKz85KUmdMbglqTMGtyR1xuCWpM4Y3JLUGYNbkjpjcEtSZwxuSeqMwS1JnTG4JakzBrckdcbglqTOGNyS1BmDW5I6Y3BLUmcMbknqjMEtSZ0xuCWpM1P66rIk24EXgDFgf1WNJjkN+DqwCtgOfLyqnp1am5Kkg6bjivu3q2pNVY22/WuAe6pqNXBP25ckTZOZWCq5FNjQtjcAH52B15CkeWuqwV3AXyZ5MMnaVltaVbvb9h5g6RRfQ5I0YEpr3MAHqmpXkl8D7kryw8GDVVVJ6kgntqBfC3DGGWdMsQ1Jmj+mdMVdVbvaz73At4BzgSeTLANoP/ce5dz1VTVaVaMjIyNTaUOS5pVJB3eSNyd5y8Ft4HeAh4GNwJVt2JXAbVNtUpL0K1NZKlkKfCvJwef571X1v5M8ANya5CrgJ8DHp96mJOmgSQd3Vf0YeM8R6k8DH5xKU5Kko/OTk5LUGYNbkjpjcEtSZwxuSeqMwS1JnTG4JakzBrckdcbglqTOGNyS1BmDW5I6Y3BLUmcMbknqjMEtSZ0xuCWpMwa3JHXG4JakzhjcktQZg1uSOmNwS1JnJgzuJDcn2Zvk4YHaaUnuSvJ4+3lqqyfJDUm2JdmS5L0z2bwkzUfDXHF/Cbj4kNo1wD1VtRq4p+0DfBhY3R5rgRunp01J0kETBndV/RXwzCHlS4ENbXsD8NGB+pdr3HeBJUmWTVezkqTJr3EvrardbXsPsLRtLwd2DIzb2WqHSbI2yaYkm/bt2zfJNiRp/pnyLyerqoCaxHnrq2q0qkZHRkam2oYkzRuTDe4nDy6BtJ97W30XsHJg3IpWkyRNk8kG90bgyrZ9JXDbQP0T7e6S84DnB5ZUJEnTYOFEA5J8DbgAOD3JTuAPgT8Gbk1yFfAT4ONt+B3AJcA24BfAJ2egZ0ma1yYM7qq6/CiHPniEsQWsm2pTkqSj85OTktQZg1uSOmNwS1JnDG5J6ozBLUmdMbglqTMGtyR1xuCWpM4Y3JLUGYNbkjpjcEtSZwxuSeqMwS1JnTG4JakzBrckdcbglqTOGNyS1BmDW5I6M2FwJ7k5yd4kDw/UPptkV5LN7XHJwLFrk2xL8liS352pxiVpvhrmivtLwMVHqF9fVWva4w6AJGcDlwG/2c75z0kWTFezkqQhgruq/gp4ZsjnuxS4papeqqonGP+293On0J8k6RBTWeP+VJItbSnl1FZbDuwYGLOz1Q6TZG2STUk27du3bwptSNL8MtngvhH4DWANsBv402N9gqpaX1WjVTU6MjIyyTYkaf6ZVHBX1ZNVNVZVB4Av8qvlkF3AyoGhK1pNkjRNJhXcSZYN7P4ecPCOk43AZUkWJzkLWA3cP7UWJUmDFk40IMnXgAuA05PsBP4QuCDJGqCA7cDvA1TVI0luBR4F9gPrqmpsZlqXpPlpwuCuqsuPUL7pdcZfB1w3laYkSUfnJyclqTMGtyR1xuCWpM4Y3JLUGYNbkjoz4V0l0nz1830/YezlX3LSkndw4puXzHY70qsMbqnZ/+LP2f5/N1BjrwDw871PMPbyLznjt/4FI3/nt2a5O+lXDG6pqQP7eWHXVg7sf3m2W5Fel2vcktQZg1uSOmNwS1JnDG5J6ozBLUmdMbglqTMGtyR1xuCWpM4Y3JLUGYNbkjozYXAnWZnk3iSPJnkkyadb/bQkdyV5vP08tdWT5IYk25JsSfLemZ6EJM0nw1xx7wc+U1VnA+cB65KcDVwD3FNVq4F72j7Ahxn/dvfVwFrgxmnvWpLmsQmDu6p2V9VDbfsFYCuwHLgU2NCGbQA+2rYvBb5c474LLEmybNo7l6R56pjWuJOsAs4B7gOWVtXudmgPsLRtLwd2DJy2s9UOfa61STYl2bRv375jbFuS5q+hgzvJKcA3gKur6qeDx6qqgDqWF66q9VU1WlWjIyMjx3KqNCOyYBEnLTn8H4e/fGYXdWBsFjqSjmyo4E6yiPHQ/mpVfbOVnzy4BNJ+7m31XcDKgdNXtJp0XFu4+E287cy/d1j92R8/yIGx/bPQkXRkw9xVEuAmYGtVfWHg0EbgyrZ9JXDbQP0T7e6S84DnB5ZUJElTNMw34LwfuAL4QZLNrfYHwB8Dtya5CvgJ8PF27A7gEmAb8Avgk9PasSTNcxMGd1V9B8hRDn/wCOMLWDfFviRJR+EnJyWpMwa3JHXG4JakzhjcktQZg1uSOmNwS1JnDG5J6ozBLUmdMbglqTMGtyR1xuCWpM4Y3JLUGYNbkjpjcEtSZwxuSeqMwS1JnTG4JakzBrckdWaYLwtemeTeJI8meSTJp1v9s0l2JdncHpcMnHNtkm1JHkvyuzM5AUmab4b5suD9wGeq6qEkbwEeTHJXO3Z9Vf37wcFJzgYuA34T+FvA3Un+dlWNTWfjkjRfTXjFXVW7q+qhtv0CsBVY/jqnXArcUlUvVdUTjH/b+7nT0awk6RjXuJOsAs4B7mulTyXZkuTmJKe22nJgx8BpO3n9oJckHYOhgzvJKcA3gKur6qfAjcBvAGuA3cCfHssLJ1mbZFOSTfv27TuWUyVpXhsquJMsYjy0v1pV3wSoqieraqyqDgBf5FfLIbuAlQOnr2i116iq9VU1WlWjIyMjU5mDJM0rw9xVEuAmYGtVfWGgvmxg2O8BD7ftjcBlSRYnOQtYDdw/fS1L0vw2zF0l7weuAH6QZHOr/QFweZI1QAHbgd8HqKpHktwKPMr4HSnrvKNEkqbPhMFdVd8BcoRDd7zOOdcB102hL0nSUfjJSUnqjMEtSZ0xuCWpMwa3JHXG4Jakzhjc0oDFbxkhJ7z2Zqs6sJ+XX3hqljqSDmdwSwOWnHUOCxa/6TW1sZd+wbNPfG+WOpIOZ3BLUmcMbknqjMEtSZ0xuCWpMwa3JHXG4JakzgzzZ12lrh04cICrr76aHTt2TDh20YKw7h+dximLF7ymfsstt/DXf3TTUK+3bt06Lrrookn1Kg3D4NacV1XcfffdbN26dcKxJ524kKvedxknLjqVqvF/kC484WW2bt3KX/zPh4Z6vY985CNT6leaiMEtHeLZV5ayed8/5pVaDMCyk57gQD0wy11Jv2JwSwPGaiGbn7uAk998yqu1PS+eyfOvnD6LXUmv5S8npUOM1Ymv2d9fi9n30opZ6kY63DBfFnxSkvuTfD/JI0k+1+pnJbkvybYkX09yYqsvbvvb2vFVMzsFafqE4qQTfvaa2qK8yPKTt81SR9Lhhrnifgm4sKreA6wBLk5yHvAnwPVV9U7gWeCqNv4q4NlWv76Nk7pwQsb4B6fezWmLdrPwwFM89dR28sJ3ePnFfbPdmvSqYb4suICDlyCL2qOAC4F/1uobgM8CNwKXtm2APwf+U5K055GOa6/sH+OLf3E3i0/8Pzz901/y11v+H1Dg21fHkaF+OZlkAfAg8E7gz4AfAc9V1f42ZCewvG0vB3YAVNX+JM8DbweO+geN9+zZw+c///lJTUCaSFXx9NNPDzV27EBxx32PT+n17rzzTp555pkpPYe0Z8+eox4bKriragxYk2QJ8C3g3VNtKslaYC3A8uXLueKKK6b6lNIRHThwgJtuuom9e/e+Ia93/vnnc/nll78hr6W56ytf+cpRjx3T7YBV9VySe4HzgSVJFrar7hXArjZsF7AS2JlkIfA24LDLnapaD6wHGB0drXe84x3H0oo0tLGxMRYsWDDxwGny1re+Fd/PmqpFixYd9dgwd5WMtCttkpwMfAjYCtwLfKwNuxK4rW1vbPu04992fVuSps8wV9zLgA1tnfsE4Naquj3Jo8AtSf4I+B5w8A853AT8tyTbgGeAy2agb0mat4a5q2QLcM4R6j8Gzj1C/UXgn05Ld5Kkw/jJSUnqjMEtSZ3xj0xpzkvCRRddxLve9a435PXOPPPMN+R1NH8Z3JrzTjjhBG644YbZbkOaNi6VSFJnDG5J6ozBLUmdMbglqTMGtyR1xuCWpM4Y3JLUGYNbkjpjcEtSZwxuSeqMwS1JnTG4JakzBrckdcbglqTODPNlwScluT/J95M8kuRzrf6lJE8k2dwea1o9SW5Isi3JliTvnelJSNJ8Mszf434JuLCqfpZkEfCdJP+rHftXVfXnh4z/MLC6Pd4H3Nh+SpKmwYRX3DXuZ213UXvU65xyKfDldt53gSVJlk29VUkSDLnGnWRBks3AXuCuqrqvHbquLYdcn2Rxqy0HdgycvrPVJEnTYKjgrqqxqloDrADOTfJ3gWuBdwP/EDgN+DfH8sJJ1ibZlGTTvn37jrFtSZq/jumukqp6DrgXuLiqdrflkJeA/wqc24btAlYOnLai1Q59rvVVNVpVoyMjI5PrXpLmoWHuKhlJsqRtnwx8CPjhwXXrJAE+CjzcTtkIfKLdXXIe8HxV7Z6R7iVpHhrmrpJlwIYkCxgP+lur6vYk304yAgTYDPzLNv4O4BJgG/AL4JPT37YkzV8TBndVbQHOOUL9wqOML2Dd1FuTJB2Jn5yUpM4Y3JLUGYNbkjpjcEtSZwxuSeqMwS1JnTG4JakzBrckdcbglqTOGNyS1BmDW5I6Y3BLUmcMbknqjMEtSZ0xuCWpMwa3JHXG4JakzhjcktQZg1uSOmNwS1JnDG5J6ozBLUmdSVXNdg8keQF4bLb7mCGnA0/NdhMzYK7OC+bu3JxXX86sqpEjHVj4RndyFI9V1ehsNzETkmyai3Obq/OCuTs35zV3uFQiSZ0xuCWpM8dLcK+f7QZm0Fyd21ydF8zduTmvOeK4+OWkJGl4x8sVtyRpSLMe3EkuTvJYkm1Jrpntfo5VkpuT7E3y8EDttCR3JXm8/Ty11ZPkhjbXLUneO3udv74kK5Pcm+TRJI8k+XSrdz23JCcluT/J99u8PtfqZyW5r/X/9SQntvritr+tHV81m/1PJMmCJN9Lcnvbnyvz2p7kB0k2J9nUal2/F6diVoM7yQLgz4APA2cDlyc5ezZ7moQvARcfUrsGuKeqVgP3tH0Yn+fq9lgL3PgG9TgZ+4HPVNXZwHnAuva/Te9zewm4sKreA6wBLk5yHvAnwPVV9U7gWeCqNv4q4NlWv76NO559Gtg6sD9X5gXw21W1ZuDWv97fi5NXVbP2AM4H7hzYvxa4djZ7muQ8VgEPD+w/Bixr28sYv08d4L8Alx9p3PH+AG4DPjSX5ga8CXgIeB/jH+BY2Oqvvi+BO4Hz2/bCNi6z3ftR5rOC8QC7ELgdyFyYV+txO3D6IbU581481sdsL5UsB3YM7O9std4trardbXsPsLRtdznf9s/oc4D7mANza8sJm4G9wF3Aj4Dnqmp/GzLY+6vzasefB97+xnY8tP8A/GvgQNt/O3NjXgAF/GWSB5OsbbXu34uTdbx8cnLOqqpK0u2tO0lOAb4BXF1VP03y6rFe51ZVY8CaJEuAbwHvnuWWpizJR4C9VfVgkgtmu58Z8IGq2pXk14C7kvxw8GCv78XJmu0r7l3AyoH9Fa3WuyeTLANoP/e2elfzTbKI8dD+alV9s5XnxNwAquo54F7GlxCWJDl4ITPY+6vzasffBjz9Brc6jPcD/yTJduAWxpdL/iP9zwuAqtrVfu5l/P9sz2UOvReP1WwH9wPA6vab7xOBy4CNs9zTdNgIXNm2r2R8ffhg/RPtt97nAc8P/FPvuJLxS+ubgK1V9YWBQ13PLclIu9ImycmMr9tvZTzAP9aGHTqvg/P9GPDtagunx5OquraqVlTVKsb/O/p2Vf1zOp8XQJI3J3nLwW3gd4CH6fy9OCWzvcgOXAL8DePrjP92tvuZRP9fA3YDrzC+lnYV42uF9wCPA3cDp7WxYfwumh8BPwBGZ7v/15nXBxhfV9wCbG6PS3qfG/D3ge+1eT0M/LtW/3XgfmAb8D+Axa1+Utvf1o7/+mzPYYg5XgDcPlfm1ebw/fZ45GBO9P5enMrDT05KUmdme6lEknSMDG5J6ozBLUmdMbglqTMGtyR1xuCWpM4Y3JLUGYNbkjrz/wFcvIQ0TnZmVgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v0\")\n",
    "\n",
    "# gym compatibility: unwrap TimeLimit\n",
    "if hasattr(env, '_max_episode_steps'):\n",
    "    env = env.env\n",
    "\n",
    "env.reset()\n",
    "n_actions = env.action_space.n\n",
    "state_dim = env.observation_space.shape\n",
    "\n",
    "plt.imshow(env.render(\"rgb_array\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EESKk4kzh5OB"
   },
   "source": [
    "# Building the network for REINFORCE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EXfvwZ8Dh5OD"
   },
   "source": [
    "For REINFORCE algorithm, we'll need a model that predicts action probabilities given states.\n",
    "\n",
    "For numerical stability, please __do not include the softmax layer into your network architecture__.\n",
    "We'll use softmax or log-softmax where appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "fJgdQDYzh5OE"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "_jRe_ceQh5OF"
   },
   "outputs": [],
   "source": [
    "# create input variables. We only need <s, a, r> for REINFORCE\n",
    "ph_states = tf.placeholder('float32', (None,) + state_dim, name=\"states\")\n",
    "ph_actions = tf.placeholder('int32', name=\"action_ids\")\n",
    "ph_cumulative_rewards = tf.placeholder('float32', name=\"cumulative_returns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4BHBqkLOjPHh",
    "outputId": "8afe5fd7-9494-4eae-9c0c-f24a11490e4a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.layers import Dense, Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "41CgCHw-lagI",
    "outputId": "76c13225-c0d2-439a-a1bc-b4184a08705a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v2OgQvtoh5OG",
    "outputId": "8df4792d-26db-4b96-f4eb-571779403ca2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    }
   ],
   "source": [
    "#<YOUR CODE: define network graph using raw TF, Keras, or any other library you prefer>\n",
    "model = tf.keras.Sequential()\n",
    "#model.add(tf.keras.layers.InputLayer(state_dim))\n",
    "model.add(tf.keras.layers.Dense(32, activation= 'relu', input_shape=state_dim))\n",
    "model.add(tf.keras.layers.Dense(32, activation= 'relu'))\n",
    "model.add(tf.keras.layers.Dense(n_actions, activation='linear'))\n",
    "\n",
    "logits = model(ph_states)\n",
    "policy = tf.nn.softmax(logits)\n",
    "log_policy = tf.nn.log_softmax(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iftDWHub94tB",
    "outputId": "bc1dfe3c-9d99-4c27-dfc2-3f520e21ebc4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 32)                160       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 66        \n",
      "=================================================================\n",
      "Total params: 1,282\n",
      "Trainable params: 1,282\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "FGDtlR9Dh5OH"
   },
   "outputs": [],
   "source": [
    "# Initialize model parameters\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "UWqzu0wTh5OI"
   },
   "outputs": [],
   "source": [
    "def predict_probs(states):\n",
    "    \"\"\" \n",
    "    Predict action probabilities given states.\n",
    "    :param states: numpy array of shape [batch, state_shape]\n",
    "    :returns: numpy array of shape [batch, n_actions]\n",
    "    \"\"\"\n",
    "    return policy.eval({ph_states: [states]})[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RR8O8xwmh5OJ"
   },
   "source": [
    "### Play the game\n",
    "\n",
    "We can now use our newly built agent to play the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "iAO2n7ozh5OK"
   },
   "outputs": [],
   "source": [
    "def generate_session(env, t_max=1000):\n",
    "    \"\"\" \n",
    "    Play a full session with REINFORCE agent.\n",
    "    Returns sequences of states, actions, and rewards.\n",
    "    \"\"\"\n",
    "    # arrays to record session\n",
    "    states, actions, rewards = [], [], []\n",
    "    s = env.reset()\n",
    "\n",
    "    for t in range(t_max):\n",
    "        # action probabilities array aka pi(a|s)\n",
    "        action_probs = predict_probs(s)\n",
    "\n",
    "        # Sample action with given probabilities.\n",
    "        a = np.random.choice(n_actions, 1, p=action_probs)[0]\n",
    "        new_s, r, done, info = env.step(a)\n",
    "\n",
    "        # record session history to train later\n",
    "        states.append(s)\n",
    "        actions.append(a)\n",
    "        rewards.append(r)\n",
    "\n",
    "        s = new_s\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    return states, actions, rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "m4La8kswh5OL"
   },
   "outputs": [],
   "source": [
    "# test it\n",
    "states, actions, rewards = generate_session(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_lhcGRieh5OM"
   },
   "source": [
    "### Computing cumulative rewards\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "G_t &= r_t + \\gamma r_{t + 1} + \\gamma^2 r_{t + 2} + \\ldots \\\\\n",
    "&= \\sum_{i = t}^T \\gamma^{i - t} r_i \\\\\n",
    "&= r_t + \\gamma * G_{t + 1}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "mu-eb1Pph5ON"
   },
   "outputs": [],
   "source": [
    "from inspect import GEN_RUNNING\n",
    "def get_cumulative_rewards(rewards,  # rewards at each step\n",
    "                           gamma=0.99  # discount for reward\n",
    "                           ):\n",
    "    \"\"\"\n",
    "    Take a list of immediate rewards r(s,a) for the whole session \n",
    "    and compute cumulative returns (a.k.a. G(s,a) in Sutton '16).\n",
    "    \n",
    "    G_t = r_t + gamma*r_{t+1} + gamma^2*r_{t+2} + ...\n",
    "\n",
    "    A simple way to compute cumulative rewards is to iterate from the last\n",
    "    to the first timestep and compute G_t = r_t + gamma*G_{t+1} recurrently\n",
    "\n",
    "    You must return an array/list of cumulative rewards with as many elements as in the initial rewards.\n",
    "    \"\"\"\n",
    "    G = []\n",
    "    G_t = 0\n",
    "    for t in reversed(range(len(rewards))):\n",
    "      G_t = gamma * G_t + rewards[t]\n",
    "      G.append(G_t)\n",
    "\n",
    "    G.reverse() \n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FqQ5W68Wh5OO",
    "outputId": "4017acad-edef-4b9e-f385-a3ae4d9b769c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "looks good!\n"
     ]
    }
   ],
   "source": [
    "assert len(get_cumulative_rewards(range(100))) == 100\n",
    "assert np.allclose(\n",
    "    get_cumulative_rewards([0, 0, 1, 0, 0, 1, 0], gamma=0.9),\n",
    "    [1.40049, 1.5561, 1.729, 0.81, 0.9, 1.0, 0.0])\n",
    "assert np.allclose(\n",
    "    get_cumulative_rewards([0, 0, 1, -2, 3, -4, 0], gamma=0.5),\n",
    "    [0.0625, 0.125, 0.25, -1.5, 1.0, -4.0, 0.0])\n",
    "assert np.allclose(\n",
    "    get_cumulative_rewards([0, 0, 1, 2, 3, 4, 0], gamma=0),\n",
    "    [0, 0, 1, 2, 3, 4, 0])\n",
    "print(\"looks good!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8biUDBUEh5OQ"
   },
   "source": [
    "#### Loss function and updates\n",
    "\n",
    "We now need to define objective and update over policy gradient.\n",
    "\n",
    "Our objective function is\n",
    "\n",
    "$$ J \\approx  { 1 \\over N } \\sum_{s_i,a_i} G(s_i,a_i) $$\n",
    "\n",
    "REINFORCE defines a way to compute the gradient of the expected reward with respect to policy parameters. The formula is as follows:\n",
    "\n",
    "$$ \\nabla_\\theta \\hat J(\\theta) \\approx { 1 \\over N } \\sum_{s_i, a_i} \\nabla_\\theta \\log \\pi_\\theta (a_i \\mid s_i) \\cdot G_t(s_i, a_i) $$\n",
    "\n",
    "We can abuse Tensorflow's capabilities for automatic differentiation by defining our objective function as follows:\n",
    "\n",
    "$$ \\hat J(\\theta) \\approx { 1 \\over N } \\sum_{s_i, a_i} \\log \\pi_\\theta (a_i \\mid s_i) \\cdot G_t(s_i, a_i) $$\n",
    "\n",
    "When you compute the gradient of that function with respect to network weights $\\theta$, it will become exactly the policy gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "e0_Gcd7Ph5OQ"
   },
   "outputs": [],
   "source": [
    "# This code selects the log-probabilities (log pi(a_i|s_i)) for those actions that were actually played.\n",
    "indices = tf.stack([tf.range(tf.shape(log_policy)[0]), ph_actions], axis=-1)\n",
    "log_policy_for_actions = tf.gather_nd(log_policy, indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "KSP1eukBh5OR"
   },
   "outputs": [],
   "source": [
    "# Policy objective as in the last formula. Please use reduce_mean, not reduce_sum.\n",
    "# You may use log_policy_for_actions to get log probabilities for actions taken.\n",
    "# Also recall that we defined ph_cumulative_rewards earlier.\n",
    "\n",
    "J = tf.reduce_mean(log_policy_for_actions * ph_cumulative_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ygpp724VDFHF",
    "outputId": "08080f4d-b3af-48b9-de43-e515c3033883"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Mean:0' shape=() dtype=float32>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "J"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ZmC-CC3h5OR"
   },
   "source": [
    "As a reminder, for a discrete probability distribution (like the one our policy outputs), entropy is defined as:\n",
    "\n",
    "$$ \\operatorname{entropy}(p) = -\\sum_{i = 1}^n p_i \\cdot \\log p_i $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "ElXL8HsVh5OS"
   },
   "outputs": [],
   "source": [
    "# Entropy regularization. If you don't add it, the policy will quickly deteriorate to\n",
    "# being deterministic, harming exploration.\n",
    "\n",
    "entropy = -tf.reduce_sum(policy * log_policy, 1, name=\"entropy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "rsYdNVITh5OT"
   },
   "outputs": [],
   "source": [
    "# # Maximizing X is the same as minimizing -X, hence the sign.\n",
    "loss = -(J + 0.1 * entropy)\n",
    "\n",
    "update = tf.train.AdamOptimizer().minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "MHbCkrpKh5OT"
   },
   "outputs": [],
   "source": [
    "def train_on_session(states, actions, rewards, t_max=1000):\n",
    "    \"\"\"given full session, trains agent with policy gradient\"\"\"\n",
    "    cumulative_rewards = get_cumulative_rewards(rewards)\n",
    "    update.run({\n",
    "        ph_states: states,\n",
    "        ph_actions: actions,\n",
    "        ph_cumulative_rewards: cumulative_rewards,\n",
    "    })\n",
    "    return sum(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "xU1R6YRBh5OT"
   },
   "outputs": [],
   "source": [
    "# Initialize optimizer parameters\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pONUcgZoh5OU"
   },
   "source": [
    "### The actual training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ltO9Xqinh5OU",
    "outputId": "318ee029-1228-4bf7-e16d-ad05967f3e8a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean reward: 530.230\n",
      "mean reward: 884.890\n",
      "You Win!\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    rewards = [train_on_session(*generate_session(env)) for _ in range(100)]  # generate new sessions\n",
    "\n",
    "    print(\"mean reward: %.3f\" % (np.mean(rewards)))\n",
    "\n",
    "    if np.mean(rewards) > 600:\n",
    "        print(\"You Win!\")  # but you can train even further\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ThhFXageh5OV"
   },
   "source": [
    "### Results & video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "Tr62-JOBh5OV"
   },
   "outputs": [],
   "source": [
    "# Record sessions\n",
    "\n",
    "import gym.wrappers\n",
    "\n",
    "with gym.wrappers.Monitor(gym.make(\"CartPole-v0\"), directory=\"videos\", force=True) as env_monitor:\n",
    "    sessions = [generate_session(env_monitor) for _ in range(100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 502,
     "resources": {
      "http://localhost:8080/videos/openaigym.video.1.84.video000064.mp4": {
       "data": "CjwhRE9DVFlQRSBodG1sPgo8aHRtbCBsYW5nPWVuPgogIDxtZXRhIGNoYXJzZXQ9dXRmLTg+CiAgPG1ldGEgbmFtZT12aWV3cG9ydCBjb250ZW50PSJpbml0aWFsLXNjYWxlPTEsIG1pbmltdW0tc2NhbGU9MSwgd2lkdGg9ZGV2aWNlLXdpZHRoIj4KICA8dGl0bGU+RXJyb3IgNDA0IChOb3QgRm91bmQpISExPC90aXRsZT4KICA8c3R5bGU+CiAgICAqe21hcmdpbjowO3BhZGRpbmc6MH1odG1sLGNvZGV7Zm9udDoxNXB4LzIycHggYXJpYWwsc2Fucy1zZXJpZn1odG1se2JhY2tncm91bmQ6I2ZmZjtjb2xvcjojMjIyO3BhZGRpbmc6MTVweH1ib2R5e21hcmdpbjo3JSBhdXRvIDA7bWF4LXdpZHRoOjM5MHB4O21pbi1oZWlnaHQ6MTgwcHg7cGFkZGluZzozMHB4IDAgMTVweH0qID4gYm9keXtiYWNrZ3JvdW5kOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9lcnJvcnMvcm9ib3QucG5nKSAxMDAlIDVweCBuby1yZXBlYXQ7cGFkZGluZy1yaWdodDoyMDVweH1we21hcmdpbjoxMXB4IDAgMjJweDtvdmVyZmxvdzpoaWRkZW59aW5ze2NvbG9yOiM3Nzc7dGV4dC1kZWNvcmF0aW9uOm5vbmV9YSBpbWd7Ym9yZGVyOjB9QG1lZGlhIHNjcmVlbiBhbmQgKG1heC13aWR0aDo3NzJweCl7Ym9keXtiYWNrZ3JvdW5kOm5vbmU7bWFyZ2luLXRvcDowO21heC13aWR0aDpub25lO3BhZGRpbmctcmlnaHQ6MH19I2xvZ297YmFja2dyb3VuZDp1cmwoLy93d3cuZ29vZ2xlLmNvbS9pbWFnZXMvbG9nb3MvZXJyb3JwYWdlL2Vycm9yX2xvZ28tMTUweDU0LnBuZykgbm8tcmVwZWF0O21hcmdpbi1sZWZ0Oi01cHh9QG1lZGlhIG9ubHkgc2NyZWVuIGFuZCAobWluLXJlc29sdXRpb246MTkyZHBpKXsjbG9nb3tiYWNrZ3JvdW5kOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9sb2dvcy9lcnJvcnBhZ2UvZXJyb3JfbG9nby0xNTB4NTQtMngucG5nKSBuby1yZXBlYXQgMCUgMCUvMTAwJSAxMDAlOy1tb3otYm9yZGVyLWltYWdlOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9sb2dvcy9lcnJvcnBhZ2UvZXJyb3JfbG9nby0xNTB4NTQtMngucG5nKSAwfX1AbWVkaWEgb25seSBzY3JlZW4gYW5kICgtd2Via2l0LW1pbi1kZXZpY2UtcGl4ZWwtcmF0aW86Mil7I2xvZ297YmFja2dyb3VuZDp1cmwoLy93d3cuZ29vZ2xlLmNvbS9pbWFnZXMvbG9nb3MvZXJyb3JwYWdlL2Vycm9yX2xvZ28tMTUweDU0LTJ4LnBuZykgbm8tcmVwZWF0Oy13ZWJraXQtYmFja2dyb3VuZC1zaXplOjEwMCUgMTAwJX19I2xvZ297ZGlzcGxheTppbmxpbmUtYmxvY2s7aGVpZ2h0OjU0cHg7d2lkdGg6MTUwcHh9CiAgPC9zdHlsZT4KICA8YSBocmVmPS8vd3d3Lmdvb2dsZS5jb20vPjxzcGFuIGlkPWxvZ28gYXJpYS1sYWJlbD1Hb29nbGU+PC9zcGFuPjwvYT4KICA8cD48Yj40MDQuPC9iPiA8aW5zPlRoYXTigJlzIGFuIGVycm9yLjwvaW5zPgogIDxwPiAgPGlucz5UaGF04oCZcyBhbGwgd2Uga25vdy48L2lucz4K",
       "headers": [
        [
         "content-length",
         "1449"
        ],
        [
         "content-type",
         "text/html; charset=utf-8"
        ]
       ],
       "ok": false,
       "status": 404,
       "status_text": ""
      }
     }
    },
    "id": "Ett6VzL6h5OV",
    "outputId": "01b49218-f522-474a-ae38-940b14d7911f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"640\" height=\"480\" controls>\n",
       "  <source src=\"videos/openaigym.video.1.84.video000064.mp4\" type=\"video/mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show video. This may not work in some setups. If it doesn't\n",
    "# work for you, you can download the videos and view them locally.\n",
    "\n",
    "from pathlib import Path\n",
    "from IPython.display import HTML\n",
    "\n",
    "video_names = sorted([s for s in Path('videos').iterdir() if s.suffix == '.mp4'])\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(video_names[-1]))  # You can also try other indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yIlno6xah5OW",
    "outputId": "f30e1c3c-6f9c-429c-bc77-81dd84a31218"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your average reward is 948.35 over 100 episodes\n",
      "Submitted to Coursera platform. See results on assignment page!\n"
     ]
    }
   ],
   "source": [
    "from submit import submit_cartpole\n",
    "submit_cartpole(generate_session, 'vlasovve@inbox.ru', 'LBubLnIN2YogOS9g')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8gHvsW6mh5OW"
   },
   "source": [
    "That's all, thank you for your attention!\n",
    "\n",
    "Not having enough? There's an actor-critic waiting for you in the honor section. But make sure you've seen the videos first."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "practice_reinforce.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
